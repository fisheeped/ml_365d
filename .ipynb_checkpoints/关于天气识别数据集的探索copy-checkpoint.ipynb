{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8180139A4C284CC2856D0656B3E34B80",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Declaration\n",
    ">- Env:Look the follow cell, you can double click for full view.\n",
    ">- About language:To study English for read [tensorflow API](https://tensorflow.google.cn/versions/r2.3/api_docs/python/tf).\n",
    ">- Epic:Upon finishing this Notebook, to express my great gratitude towards all those who have offered me sincere assistance and thought especially KÂêåÂ≠¶Âïä and heywhale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "B91CD54B8A39482AA7765880747E384A",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib                    3.4.3\n",
      "matplotlib-inline             0.1.3\n",
      "Pillow                        8.3.2\n",
      "tensorflow                    2.9.0\n",
      "tensorflow-cpu                2.6.2\n",
      "tensorflow-estimator          2.9.0\n",
      "tensorflow-io-gcs-filesystem  0.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep -E \"tensorflow|matplotlib|Pillow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5C262C37B11E4A8487DBE76B96BF9760",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "id": "866010E3337446EE946E5D9983779F05",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense, RandomFlip, RandomZoom,RandomRotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "79CA1C16B01F4F77A56B690665622BFD",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## View file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "DC4F566D686D4B2C80C63D3EB7887676",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View data directory\n",
    "%ls /home/mw/input/weather_photos9700/weather_photos/weather_photos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "F00A22BB292A4943B5C17FA25A7593E5",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%ls /home/mw/input/weather_photos9700/weather_photos/weather_photos/cloudy|head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FD2FEF99B5D14642BF357634342FD74E",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    " The dataset contains four sub-directories, one per class:\n",
    " ```\n",
    " weather_photos/\n",
    "\t\tcloudy/\n",
    "\t\train/\n",
    "\t\tshine/\n",
    "\t\tsunrise/\n",
    "\t\t\t\t*.jpg\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "4B66177100C84834827E436CF5C2C6BF",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imageÔºö 1125\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/mw/input/weather_photos9700/weather_photos/weather_photos/\"\n",
    "# Instantiating a pathlib object\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(\"Number of imageÔºö\",image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "30496466FD614D098BB6295F9E0E95BA",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can find the class names in the `class_names` attribute on these datasets. These correspond to the directory names in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "2477904E0C67477393DE0637E4800DD7",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cloudy', 'rain', 'shine', 'sunrise']"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "transient": {}
    }
   ],
   "source": [
    "class_names = [i.name for i in data_dir.glob('*')]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CC1ABF3E43C44F87BC190BE1D05C72E6",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Visualize the first image\n",
    "\n",
    "Here are the first image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9EFFA94FB4574ACEA4BEC583BE75A364",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "data_dir.glob(re_path) Return a generator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "465BED967AE340E5991ADF4C7DF0D934",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x400 at 0x7F83A387A950>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/465BED967AE340E5991ADF4C7DF0D934/rggdgopjoc.png\">"
      ],
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x400 at 0x7F83A3809290>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "transient": {}
    }
   ],
   "source": [
    "re_path = '%s/*.jpg'%(class_names[0])\n",
    "for i in data_dir.glob(re_path):\n",
    "    break\n",
    "img = PIL.Image.open(i)\n",
    "print(img)\n",
    "PIL.Image.open(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "05F627FE1A4E406B91CE5BB1F5989D56",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    " ## Image structure\n",
    "Get the size and mode of the image from the above , **JpegImageFile image mode=RGB size=600x400**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5BF2540D5BBA471CBE3D2E085C10738A",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Load data using a Keras utility\n",
    "\n",
    "Let's load these images off disk using the helpful `tf.keras.preprocessing.image_dataset_from_directory` utility. This will take you from a directory of images on disk to a `tf.data.Dataset` in just a couple lines of code. If you like, you can also write your own data loading code from scratch by visiting the [Load and preprocess images](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/images.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FDBF598CB58A497E82D4A28DDA1D6420",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Resource\n",
    ">- Êú¨Êñá‰∏∫[üîó365Â§©Ê∑±Â∫¶Â≠¶‰π†ËÆ≠ÁªÉËê•](https://mp.weixin.qq.com/s/k-vYaC8l7uxX51WoypLkTw) ‰∏≠ÁöÑÂ≠¶‰π†ËÆ∞ÂΩïÂçöÂÆ¢\n",
    ">- ÂèÇËÄÉÊñáÁ´†Âú∞ÂùÄÔºö [üîóÊ∑±Â∫¶Â≠¶‰π†100‰æã-Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÂ§©Ê∞îËØÜÂà´ | Á¨¨5Â§©](https://mtyjkh.blog.csdn.net/article/details/117186183)\n",
    ">- ÂèÇËÄÉcolabÂú∞ÂùÄÔºö[üîóimages_classification](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb#scrollTo=qFYx7Z2SIjr8)\n",
    ">- ÂèÇËÄÉtensorflowAPI:[üîótf.keras.preprocessing.image_dataset_from_directory](https://tensorflow.google.cn/versions/r2.3/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CC84F9CABFDB485A8A4DC47A86F53124",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Create a dataset\n",
    "Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "id": "00D50B49DBD3452A8B24F316925D3CB7",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set 32 times for wrap\n",
    "batch_size = 32\n",
    "img_height = 192 \n",
    "img_width = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "C94869387539483D924035C87F895FEC",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "It's good practice to use a validation split when developing your model. Let's use 80% of the images for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "6FE3144544FD4BC99D2196D3B9B986A4",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1125 files belonging to 4 classes.\n",
      "Using 900 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 13:29:17.137356: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-11 13:29:17.137387: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-11 13:29:17.137406: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (modelwhale): /proc/driver/nvidia/version does not exist\n",
      "2022-08-11 13:29:17.137649: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=3,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "id": "74511F42184C44CCBC08816E0A4E96ED",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1125 files belonging to 4 classes.\n",
      "Using 225 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=3,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "74067B7C295940E881DE1882D00BAB84",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can find the class names in the class_names attribute on these datasets. These correspond to the directory names in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "4194EFB180754F20909F78EC377C49AB",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cloudy', 'rain', 'shine', 'sunrise']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "18A73AC8C06E4BDEAEE2C8D779237F13",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Visualize the data\n",
    "\n",
    "Here are the first nine images from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "BB480E11E8374B7B80AD869A9B9824DE",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/BB480E11E8374B7B80AD869A9B9824DE/rggdh267ok.png\">"
      ],
      "text/plain": [
       "<Figure size 1440x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(20):\n",
    "    ax = plt.subplot(5, 10, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BF9FC320F7CE442DB3EFE812F962CC65",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You will train a model using these datasets by passing them to Model.fit in a moment. If you like, you can also manually iterate over the dataset and retrieve batches of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "id": "A700BF89E94245D79CA7F865518DE479",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 192, 192, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2126E17819D042A080F39F152E8E657B",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The `image_batch` is a tensor of the shape `(32, 192, 192, 3)`. This is a batch of 32 images of shape `192x192x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n",
    "\n",
    "You can call `.numpy()` on the `image_batch` and `labels_batch` tensors to convert them to a `numpy.ndarray`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "32D3631ECB9B49E0A8D2236BA28DDAFE",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n",
    "\n",
    "- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "- `Dataset.prefetch` overlaps data preprocessing and model execution while training.\n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the *Prefetching* section of the [Better performance with the tf.data API](.https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data_performance.ipynb) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "id": "F14C34EBC23A4E4B981CBC3222003832",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(256).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BACEE506F9D34B15ACC744D5CE12CF6F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Standardize the data\n",
    "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small.\n",
    "\n",
    "Here, you will standardize values to be in the `[0, 1]` range by using `tf.keras.layers.Rescaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "3F5FE56A93A048498FC4481D10381B6A",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "id": "8CA1939D0A6845838D1FE92A69841936",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.image_preprocessing.Rescaling at 0x7f839015c5d0>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "transient": {}
    }
   ],
   "source": [
    "normalization_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BB23130034FB440F82473009F750EA2B",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5B81400BE30840D7A47285320B9A413E",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Overfitting generally occurs when there are a small number of training examples. [Data augmentation](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb) takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "You will implement data augmentation using the following Keras preprocessing layers: `tf.keras.layers.RandomFlip`, `tf.keras.layers.RandomRotation`, and `tf.keras.layers.RandomZoom`. These can be included inside your model like other layers, and run on the GPU. But I use tensorflow.keras.preprocessing.image.ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "id": "BE7413CCECAA4EBB82E26DEEB19362FB",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.5),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "id": "F44FC93EA05E4022B5A553AA8C68B16E",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/F44FC93EA05E4022B5A553AA8C68B16E/rggdjlth0g.png\">"
      ],
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "id": "7613E1214DD24D58B954B9825B243430",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG16(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.data_aug1 = RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3))\n",
    "        self.data_aug2 = RandomRotation(0.1)\n",
    "        self.data_aug3 = RandomZoom(0.1) \n",
    "        self.c1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same')  # Âç∑ÁßØÂ±Ç1\n",
    "        self.b1 = BatchNormalization()  \n",
    "        self.a1 = Activation('relu') \n",
    "        self.c2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', )\n",
    "        self.b2 = BatchNormalization()  \n",
    "        self.a2 = Activation('relu')  \n",
    "        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d1 = Dropout(0.2) \n",
    "\n",
    "        self.c3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')\n",
    "        self.b3 = BatchNormalization() \n",
    "        self.a3 = Activation('relu')  \n",
    "        self.c4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same')\n",
    "        self.b4 = BatchNormalization() \n",
    "        self.a4 = Activation('relu')  \n",
    "        self.p2 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d2 = Dropout(0.2)  \n",
    "\n",
    "        self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b5 = BatchNormalization()  \n",
    "        self.a5 = Activation('relu')  \n",
    "        self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b6 = BatchNormalization()  \n",
    "        self.a6 = Activation('relu')  \n",
    "        self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b7 = BatchNormalization()\n",
    "        self.a7 = Activation('relu')\n",
    "        self.p3 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d3 = Dropout(0.2)\n",
    "\n",
    "        self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b8 = BatchNormalization()  \n",
    "        self.a8 = Activation('relu')  \n",
    "        self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b9 = BatchNormalization()  \n",
    "        self.a9 = Activation('relu')  \n",
    "        self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b10 = BatchNormalization()\n",
    "        self.a10 = Activation('relu')\n",
    "        self.p4 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d4 = Dropout(0.2)\n",
    "\n",
    "        self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b11 = BatchNormalization()  \n",
    "        self.a11 = Activation('relu')  \n",
    "        self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b12 = BatchNormalization()  \n",
    "        self.a12 = Activation('relu')  \n",
    "        self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b13 = BatchNormalization()\n",
    "        self.a13 = Activation('relu')\n",
    "        self.p5 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d5 = Dropout(0.2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(512, activation='relu')\n",
    "        self.d6 = Dropout(0.2)\n",
    "        self.f2 = Dense(512, activation='relu')\n",
    "        self.d7 = Dropout(0.2)\n",
    "        self.f3 = Dense(4, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.data_aug1(x)\n",
    "        x = self.data_aug2(x)\n",
    "        x = self.data_aug3(x)\n",
    "        \n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = self.c3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.d2(x)\n",
    "\n",
    "        x = self.c5(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.b6(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.c7(x)\n",
    "        x = self.b7(x)\n",
    "        x = self.a7(x)\n",
    "        x = self.p3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = self.c8(x)\n",
    "        x = self.b8(x)\n",
    "        x = self.a8(x)\n",
    "        x = self.c9(x)\n",
    "        x = self.b9(x)\n",
    "        x = self.a9(x)\n",
    "        x = self.c10(x)\n",
    "        x = self.b10(x)\n",
    "        x = self.a10(x)\n",
    "        x = self.p4(x)\n",
    "        x = self.d4(x)\n",
    "\n",
    "        x = self.c11(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.a11(x)\n",
    "        x = self.c12(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.a12(x)\n",
    "        x = self.c13(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.a13(x)\n",
    "        x = self.p5(x)\n",
    "        x = self.d5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d6(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.d7(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "id": "F4FF18F94B6949E48B857D9D663A5E5A",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "id": "73472FAD9517458C89798DC2BBCC2B72",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "id": "7F9593124BFF44FDB17C9684BCA7EF14",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28/29 [===========================>..] - ETA: 45s - loss: 4.2795 - accuracy: 0.3030 "
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "id": "4BBEC5134E2F4287BDB66AE51913C1B9",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_1 (Sequential)   (None, 192, 192, 3)       0         \n",
      "                                                                 \n",
      " rescaling_2 (Rescaling)     (None, 192, 192, 3)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 190, 190, 16)      448       \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 95, 95, 16)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 93, 93, 32)        4640      \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 46, 46, 32)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 44, 44, 64)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 123904)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               15859840  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,883,940\n",
      "Trainable params: 15,883,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)), # Âç∑ÁßØÂ±Ç1ÔºåÂç∑ÁßØÊ†∏3*3  \n",
    "    layers.AveragePooling2D((2, 2)),               # Ê±†ÂåñÂ±Ç1Ôºå2*2ÈááÊ†∑\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),  # Âç∑ÁßØÂ±Ç2ÔºåÂç∑ÁßØÊ†∏3*3\n",
    "    layers.AveragePooling2D((2, 2)),               # Ê±†ÂåñÂ±Ç2Ôºå2*2ÈááÊ†∑\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  # Âç∑ÁßØÂ±Ç3ÔºåÂç∑ÁßØÊ†∏3*3\n",
    "    layers.Dropout(0.3),  \n",
    "    \n",
    "    layers.Flatten(),                       # FlattenÂ±ÇÔºåËøûÊé•Âç∑ÁßØÂ±Ç‰∏éÂÖ®ËøûÊé•Â±Ç\n",
    "    layers.Dense(128, activation='relu'),   # ÂÖ®ËøûÊé•Â±ÇÔºåÁâπÂæÅËøõ‰∏ÄÊ≠•ÊèêÂèñ\n",
    "    layers.Dense(num_classes)               # ËæìÂá∫Â±ÇÔºåËæìÂá∫È¢ÑÊúüÁªìÊûú\n",
    "])\n",
    "\n",
    "model.summary()  # ÊâìÂç∞ÁΩëÁªúÁªìÊûÑ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "id": "863F63C4705D4C458B355B3147A06B1A",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ËÆæÁΩÆ‰ºòÂåñÂô®\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "id": "1E56DD05DF74452AA3C56C4E015531B1",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 41s 1s/step - loss: 0.9672 - accuracy: 0.6011 - val_loss: 0.7063 - val_accuracy: 0.7378\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.5388 - accuracy: 0.7989 - val_loss: 0.6851 - val_accuracy: 0.7244\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.4716 - accuracy: 0.8111 - val_loss: 0.5917 - val_accuracy: 0.8533\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.4084 - accuracy: 0.8433 - val_loss: 0.5063 - val_accuracy: 0.8800\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 41s 1s/step - loss: 0.4067 - accuracy: 0.8411 - val_loss: 0.5118 - val_accuracy: 0.8489\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.3507 - accuracy: 0.8722 - val_loss: 0.4920 - val_accuracy: 0.8489\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.3583 - accuracy: 0.8733 - val_loss: 0.5790 - val_accuracy: 0.8222\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 41s 1s/step - loss: 0.3381 - accuracy: 0.8778 - val_loss: 0.4361 - val_accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 41s 1s/step - loss: 0.3257 - accuracy: 0.8700 - val_loss: 0.5076 - val_accuracy: 0.8756\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 40s 1s/step - loss: 0.2925 - accuracy: 0.8844 - val_loss: 0.5841 - val_accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DBFC52451AF24D7980BCBE4878269193",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Visualize training results\n",
    "\n",
    "After applying data augmentation and `tf.keras.layers.Dropout`, there is less overfitting than before, and training and validation accuracy are closer aligned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "id": "862709F6CC15438EB4FDEEE39971F00E",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/862709F6CC15438EB4FDEEE39971F00E/rgcw846qq.png\">"
      ],
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "id": "6CAFDB97AED24EDD82D1ADEEC51762DC",
    "jupyter": {},
    "notebookId": "62ee591d5938e7ce89e415ae",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/6CAFDB97AED24EDD82D1ADEEC51762DC/rgcveb1rqz.png\">"
      ],
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
